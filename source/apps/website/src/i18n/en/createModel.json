{
  "wizard": {
    "stepNumberLabel": "Step {{stepNumber}}",
    "collapsedStepsLabel": "Step {{stepNumber}} of {{stepsCount}}",
    "cancelButton": "Cancel",
    "previousButton": "Previous",
    "nextButton": "Next",
    "submitButton": "Train your model",
    "optional": "optional",
    "stepTitles": {
      "modelInfo": "Specify model name and environment",
      "trainingType": "Choose vehicle sensors and hyperparameters",
      "actionSpace": "Define action space",
      "rewardAlgorithm": "Customize reward function",
      "agentSelection": "Choose a shell"
    }
  },
  "modelInfo": {
    "overviewSection": {
      "header": "Overview",
      "rlDiagramAlt": "Diagram",
      "overviewDescription1": "In reinforcement learning for AWS DeepRacer, an <strong>agent</strong> (vehicle) learns from an <strong>environment</strong> (a track) by interacting with it and receiving rewards for performing specific actions.",
      "overviewDescription2": "The model training process will simulate multiple experiences of the vehicle on the track in an attempt to find a <strong>policy</strong> (a function mapping the agent's current state to a driving decision) which maximizes the average total reward the agent experiences.",
      "overviewDescription3": "After training, you will be able to evaluate the model's performance in a new environment, deploy the model to a physical vehicle, or submit the model to a virtual circuit."
    },
    "trainingDetailsSection": {
      "header": "Training details",
      "modelNamePlaceholder": "TopModel-reInventTrack",
      "modelNameLabel": "Model name",
      "modelNameContraintText": "The model name can have up to 64 characters. Valid characters: A-Z, a-z, 0-9, and hyphens (-). No spaces or underscores (_).",
      "modelNameError": "The model name must have at least 1 non-whitespace character.",
      "modelNameLengthError": "The model name has too many characters.",
      "modelDescriptionLabel": "Model description <i>- optional</i>",
      "modelDescription": "Model description",
      "modelDescriptionPlaceholder": "Log details for quick reference",
      "modelDescriptionLengthError": "The model description has too many characters.",
      "modelDescriptionHint": "Model description can have up to 255 characters.",
      "modelDescriptionError": "The model description must have at least 1 non-whitespace character.",
      "raceType": "Race type",
      "chooseARaceType": "Choose a race type",
      "timeTrialLabel": "Time trial",
      "timeTrialDescription": "Race against the clock.",
      "objectAvoidanceLabel": "Object avoidance",
      "objectAvoidanceDescription": "Complete the fastest lap, skillfully avoiding objects on the track."
    },
    "trackSelectionSection": {
      "header": "Environment simulation",
      "trackItemsCount": "({{count}})",
      "description": "Simulated environment emulates a track to train your model.",
      "findTrackLabel": "Find track",
      "findTrackAriaLabel": "Filter tracks",
      "counterClockwise": "Counterclockwise",
      "clockwise": "Clockwise",
      "sortByLabel": "Sort by:",
      "lengthShortestToLongest": "Length: Shortest to longest",
      "lengthLongestToShortest": "Length: Longest to shortest",
      "difficultyMostToLeast": "Difficulty: Most to least",
      "difficultyLeastToMost": "Difficulty: Least to most",
      "trackDirectionLabel": "Track direction",
      "trackDirectionDescription": "Select the direction in which you wish to race"
    },
    "objectAvoidanceConfig": {
      "box": "Box",
      "boxAlt": "Box icon",
      "objectImageAlt": "Object positions image",
      "objectLocationTitle": "Choose your criteria for how DeepRacer learns to avoid object",
      "objectType": "Object Type",
      "obstacle": "Obstacle {{count}}",
      "fixedLocation": "Fixed location",
      "fixedLocationDescription": "Objects are evenly distributed in fixed locations across two lanes along the track. The objects' positions do not change from episode to episode. Training tends to converge faster when obstacles are in fixed locations. But such models might overfit and may not generalize well to real-world tracks",
      "randomizedLocation": "Randomized location",
      "randomizedLocationDescription": "Objects are randomly distributed across two lanes along the track at the beginning of each episode. Training usually takes longer to converge for models to avoid obstacles in random locations that change from episode to episode. However, the trained models are more robust and generalize better to real-world tracks.",
      "outsideLane": "Outside lane",
      "insideLane": "Inside lane",
      "numberOfObjectsLabel": "Number of objects on a track",
      "numberOfObjectsDescription": "Increasing objects on tracks may increase the time for models to converge, but resulting models are more robust.",
      "lanePlacement": "Lane placement",
      "trackPercentage": "Location (%) between start and finish",
      "minimumNumberOfObjectsError": "Minimum number of objects must be 1.",
      "maximumNumberOfObjectsError": "Maximum number of objects is 6.",
      "trackPercentageGap": "Location of obstacles must be at least 0.13 away from one another.",
      "trackPercentageMinError": "The obstacle location on the track is less than 0.07. Choose a percentage between 0.07-0.90.",
      "trackPercentageMaxError": "The obstacle location on the track is greater than 0.90. Choose a percentage between 0.07-0.90."
    }
  },
  "vehicleInfo": {
    "configureVehicleHeader": "Sensors",
    "configureVehicleDescription": "Configure your vehicle with one or more sensors, choose a neural network, topology, and customize the action space to meet your racing criteria. Customize your vehicle's appearance for personalized visualization in training.",
    "camera": "Camera",
    "cameraSensorExpandableHeader": "Benefits of the front-facing camera",
    "cameraSensorDescription": "Objects are evenly distributed in fixed locations across two lanes along the track. The objects' positions do not change from episode to episode. Training tends to converge faster when obstacles are in fixed locations. But such models might overfit and may not generalize well to real-world tracks.",
    "frontFacingCameraContent": "Single camera is the lowest cost sensor solution and is good enough for finishing simple tasks such as time trial. It requires simple neural network, which means trainings can converge faster. However, the single-lens camera only may not be sufficient to handle complex tasks such as avoiding obstacles on random locations and head-to-head racing.",
    "stereoCamera": "Stereo camera",
    "stereoCameraExpandableHeader": "Benefits of the stereo camera",
    "stereoCameraDescription": "Objects are randomly distributed across two lanes along the track at the beginning of each episode. Training usually takes longer to converge for models to avoid obstacles in random locations that change from episode to episode. However, the trained models are more robust and generalize better to real-world tracks.",
    "stereoCameraContent": "Stereo camera, made of two single-lens cameras, enables depth sensing and is valuable to avoid crashing into obstacles or other vehicles, especially in dynamic environments.",
    "sensorsHeader": "Sensors",
    "lidar": "LIDAR sensor",
    "lidarDescription": "Configure your vehicle with one or more sensors, choose a neural topology, and customize the action space to meet your racing criteria. Customize your vehicle's appearance for personalized visualization in training.",
    "ppo": "PPO",
    "ppoDescription": "A state-of-the-art policy gradient algorithm which uses two neural networks during training - a policy network and a value network.",
    "sac": "SAC",
    "sacDescription": "Not limiting itself to seeking only the maximum of lifetime rewards, this algorithm embraces exploration, incentivizing entropy in its pursuit of optimal policy.",
    "sensorModification": "Sensor modifications",
    "sensorModificationDescription": "Swap sensors to improve your DeepRacer's racing performance.",
    "addOnSensor": "Add-on sensor",
    "hyperparametersHeader": "Training algorithm and hyperparameters",
    "hyperparametersLabel": "Hyperparameters",
    "gradientDescent": "Gradient descent batch size",
    "numberOfEpochs": "Number of epochs",
    "numberOfEpochsDescription": "Integer between 3 and 10",
    "learningRate": "Learning rate",
    "learningRateDescription": "Real number between 0.00000001 (1e-8) and 0.001 (1e-3).",
    "sacAlpha": "SAC alpha (Î±) value",
    "sacAlphaDescription": "Real number between 0 and 1.",
    "entropy": "Entropy",
    "entropyDescription": "Real number between 0 and 1.",
    "discountFactor": "Discount factor",
    "discountFactorDescription": "Real number between 0 and 1.",
    "lossType": "Loss type",
    "meanSquaredError": "Mean squared error",
    "huber": "Huber",
    "numberOfEpisodes": "Number of experience episodes between each policy-updating iteration",
    "numberOfEpisodesDescription": "Integer between 5 and 100."
  },
  "actionSpace": {
    "actionSpaceImportant": "Why is action space important?",
    "actionSpaceImportantDescription": "In reinforcement learning, the set of all valid actions, or choices, available to an agent as it interacts with an environment is called an <i>action space</i>. In the DeepRacer on AWS console, you can train agents in either a <i>discrete</i> or <i>continuous</i> action space.<br/><br/> When training an DeepRacer model, the action space defines what speed and steering angle combinations are available to the agent. An <i>action</i> is a single speed and steering angle combination or choice an agent can make.",
    "selectActionSpaceHeader": "Select action space",
    "actionSpaces": "Action spaces",
    "continuousActionSpace": "Continuous action space",
    "continuousActionSpaceDescription": "A continuous action space allows the agent to select an action from a range of values for each state.",
    "discreteActionSpace": "Discrete action space",
    "discreteActionSpaceDescription": "A discrete action space represents all of the agent's possible actions for each state in a finite set.",
    "continuousSection": {
      "defineHeader": "Define continuous action space",
      "continuousDescription": "In a continuous action space setting, the agent learns to pick the optimal speed and steering values from the min/max bounds you provide through training. Providing a range of values for the model to pick from seems to be the better option but the agent has to train longer to learn to choose the optimal actions.",
      "steeringAngle": "Steering angle",
      "steeringAngleDescription": "The steering angle determines the range of steering angles in which the front wheels of your agent can turn.",
      "speed": "Speed",
      "speedDescription": "The speed determines how fast your agent can drive. Min/max speed defines the range of speeds available to the agent while training.",
      "leftSteeringAngle": "Left steering angle",
      "leftSteeringAngleConstraintText": "Values are between 0 and 30.",
      "rightSteeringAngle": "Right steering angle",
      "rightSteeringAngleConstraintText": "Values are between -30 and 0.",
      "minimumSpeed": "Minimum speed",
      "speedConstraintText": "Values are between 0.1 and 4.",
      "maximumSpeed": "Maximum speed",
      "resetButton": "Reset to default values",
      "graphLabel": "Dynamic sector graph",
      "graphHelpText": "Select and drag an arrow to change the steering angle and speed.",
      "graphImageAlt": "Action space car background"
    },
    "discreteSection": {
      "deleteModal": {
        "cancel": "Cancel",
        "delete": "Delete",
        "header": "Deleting action(s)",
        "description": "Are you sure you want to delete the selected action(s)?"
      },
      "advancedModal": {
        "cancel": "Cancel",
        "disable": "Disable advanced configuration",
        "header": "Do you want to disable advanced configuration?",
        "description": "You made changes to your action space. If you disable advanced configuration now, you will lose your changes. Are you sure you want to disable advanced configuration?"
      },
      "defineHeader": "Define discrete action space",
      "steeringAngle": "Steering angle",
      "steeringAngleDescription": "The steering angle determines the range of steering angles in which the front wheels of your agent can turn.",
      "steeringAngleGranularity": "Steering angle granularity",
      "maxSteeringAngle": "Maximum steering angle",
      "maxSteeringAngleConstraintText": "Max values are between 1 and 30.",
      "speed": "Speed",
      "speedDescription": "The speed determines how fast your agent can drive. For the agent to be able to drive faster, set a higher speed. On a given track, you must balance the desire for speed against the concern for keeping the agent on the track while it maneuvers curves at a high speed. <br /> <br /> Pro tip: The higher the speed limit and more actions, the vehicle has a better chance of driving faster, but the model may take longer to converge.",
      "speedGranularity": "Speed granularity",
      "steeringLabel": "Steering",
      "maxSpeed": "Maximum speed",
      "maxSpeedConstraintText": "Max values are between 0.1 and 4.",
      "advancedConfigAlert": "Toggle on Advanced configuration to gain a competitive advantage. You can now fine tune your discrete action space with custom steering angle and speed sets.",
      "clonedModelAlert": "In a cloned model, you can change the steering angle and speed values but you cannot add or remove any actions.",
      "actionList": "Action list",
      "advancedConfig": "Advanced configuration",
      "action": "Action",
      "steeringAngleChoose": "Choose between -30 and 30",
      "speedChoose": "Choose between 0.1 and 4",
      "degrees": "degrees",
      "metersPerSecond": "m/s",
      "addAction": "Add an action",
      "addActionHint_zero": "You have reached the limit of {{maxLimit}} action.",
      "addActionHint_one": "A new action will be added with the values of the last action in the table. You can add up to {{remaining}} more actions.",
      "addActionHint_other": "A new action will be added with the values of the last action in the table. You can add up to {{remaining}} more actions.",
      "radialPolarGraph": "Radial polar graph",
      "carImageAlt": "Action space car background",
      "selectedAction": "Selected action",
      "selectActionHint": "Select and drag an arrow to change the steering angle and speed."
    }
  },
  "testRewardFunction": {
    "rewardFunctionHeader": "Reward function",
    "rewardFunctionDescription": "The reward function describes immediate feedback (as a score for reward or penalty) when the vehicle take an action to move from a given position on the track to a new position. Its purpose is to encourage the vehicle to make moves along the track to reach its destination quickly. The model training process will attempt to find a policy which maximizes the average total reward the vehicle experiences.",
    "codeEditor": "Code editor",
    "rewardFunctonExamples": "Reward function examples",
    "reset": "Reset",
    "validate_zero": "Validate",
    "validate_one": "Wait 1 sec",
    "validate_other": "Wait {{seconds}} sec",
    "rewardFunctionValidated": "Your reward function passed validation.",
    "codeExamples": "Reward function examples",
    "useCode": "Use code",
    "timeTrialFollowCenter": "Time trial - follow the center line (Default)",
    "timeTrialFollowCenterDescription": "This example determines how far away the agent is from the center line and gives higher reward if it is closer to the center of the track. It will incentivize the agent to closely follow the center line.",
    "timeTrialStayInBorder": "Time trial - stay inside the two borders",
    "timeTrialStayInBorderDescription": "This example simply gives high rewards if the agent stays inside the borders and lets the agent figure out what is the best path to finish a lap. It is easy to program and understand, but will be likely to take longer time to converge.",
    "timeTrialPreventZigzag": "Time trial - prevent zig-zag",
    "timeTrialPreventZigzagDescription": "This example incentivizes the agent to follow the center line but penalizes with lower reward if it steers too much, which will help prevent zig-zag behavior. The agent will learn to drive smoothly in the simulator and likely display the same behavior when deployed in the physical vehicle.",
    "objectAvoidanceExample": "Object avoidance - stay on one lane and not crashing (default for OA)",
    "objectAvoidanceExampleDescription": "We consider two factors in this reward function. First, reward the agent to stay inside two borders. Second, penalize the agent for getting too close to the next object to avoid crashes. The total reward is calculated with weighted sum of the two factors. The example emphasize more on avoiding crashes but you can play with different weights.",
    "codeEditorStrings": {
      "loadingState": "Loading code editor",
      "errorState": "There was an error loading the code editor.",
      "errorStateRecovery": "Retry",
      "editorGroupAriaLabel": "Code editor",
      "statusBarGroupAriaLabel": "Status bar",
      "cursorPosition": "Ln {{row}}, Col {{column}}",
      "errorsTab": "Errors",
      "warningsTab": "Warnings",
      "preferencesButtonAriaLabel": "Preferences",
      "paneCloseButtonAriaLabel": "Close",
      "preferencesModalHeader": "Preferences",
      "preferencesModalCancel": "Cancel",
      "preferencesModalConfirm": "Confirm",
      "preferencesModalWrapLines": "Wrap lines",
      "preferencesModalTheme": "Theme",
      "preferencesModalLightThemes": "Light themes",
      "preferencesModalDarkThemes": "Dark themes"
    }
  },
  "stopCondition": {
    "content1": "Set the condition for your model training to stop. To avoid run-away jobs, you can limit the length of a job to within a maximum time period (<strong>Maximum time</strong>).",
    "content2": "The training will stop when the specified criteria is met. When your model has stopped training, you will be able to clone your model to start training again using new parameters.",
    "maximumTimeError": "The maximum training time is 1440 minutes.",
    "maximumTimeLabel": "Maximum time",
    "maximumTimeInfo": "Maximum time must be between 10 and 1440 minutes.",
    "minimumTimeError": "The minimum training time is 10 minutes.",
    "title": "Stop condition"
  },
  "carShellSelection": {
    "vehicleShell": {
      "description": "A vehicle shell has no performance impact, and is cosmetic only",
      "title": "Vehicle shell",
      "trimTitle": "Trim colors"
    },
    "autoSubmit": {
      "content1": "Submit this model to the following race after training completion",
      "content2": "Select leaderboard to submit model to",
      "title": "Automatically submit to the DeepRacer race"
    }
  },
  "requiredError": "Required"
}
